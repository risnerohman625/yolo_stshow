import av
import cv2
import torch
import numpy as np
from PIL import Image
import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
from torchvision import transforms
from ultralytics import YOLO

# â€”â€”â€”â€”â€”â€”â€”â€” é¡µé¢é…ç½® â€”â€”â€”â€”â€”â€”â€”â€”
st.set_page_config(
    page_title="æ™ºèƒ½è´¨æ£€ç³»ç»Ÿ", 
    page_icon="ğŸ”", 
    layout="wide"
)

st.title("ğŸ” æ™ºèƒ½è´¨æ£€ï¼ˆWebRTC ç‰ˆï¼‰")

# â€”â€”â€”â€”â€”â€”â€”â€” æ¨¡å‹åŠ è½½ â€”â€”â€”â€”â€”â€”â€”â€”
@st.cache_resource
def load_models():
    # YOLO æ£€æµ‹æ¨¡å‹
    yolo = YOLO("runs/detect/defect_v8s/weights/best.pt")
    # CNN åˆ†ç±»æ¨¡å‹ï¼ˆç¤ºä¾‹ç”¨ ResNet18ï¼‰
    cnn = torch.hub.load("pytorch/vision:v0.14.0", "resnet18", pretrained=False)
    cnn.fc = torch.nn.Linear(cnn.fc.in_features, 3)
    cnn.load_state_dict(torch.load("defect_cnn.pth", map_location="cpu"))
    cnn.eval()
    return yolo, cnn

yolo_model, cnn_model = load_models()
CLASS_NAMES = ["dong", "que", "normal"]

def cnn_classify(crop: np.ndarray) -> str:
    tf = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
    ])
    tensor = tf(Image.fromarray(crop)).unsqueeze(0)
    with torch.no_grad():
        out = cnn_model(tensor)
    return CLASS_NAMES[int(out.argmax())]

# â€”â€”â€”â€”â€”â€”â€”â€” è§†é¢‘å¸§å¤„ç†å™¨ â€”â€”â€”â€”â€”â€”â€”â€”
class VideoTransformer(VideoTransformerBase):
    def __init__(self):
        self.conf_th = 0.5

    def transform(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        # YOLO é¢„æµ‹
        result = yolo_model.predict(img, conf=self.conf_th, verbose=False)[0]
        for box, cls, conf in zip(result.boxes.xyxy, result.boxes.cls, result.boxes.conf):
            x1, y1, x2, y2 = map(int, box.cpu().numpy())
            label = result.names[int(cls.cpu().numpy())]
            # å¦‚æœæ˜¯å¯ç–‘ç¼ºé™·ç±»åˆ«ï¼Œå†åš CNN è¿›ä¸€æ­¥åˆ†ç±»
            if label == "defect":
                crop = img[y1:y2, x1:x2]
                sublabel = cnn_classify(crop)
                label = f"{label}/{sublabel}"
            # ç»˜åˆ¶æ¡†å’Œæ–‡å­—
            cv2.rectangle(img, (x1,y1), (x2,y2), (0,255,0), 2)
            cv2.putText(
                img, f"{label} {conf:.2f}", 
                (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 
                0.6, (255,255,255), 2
            )
        return av.VideoFrame.from_ndarray(img, format="bgr24")

# â€”â€”â€”â€”â€”â€”â€”â€” ä¾§æ è®¾ç½® â€”â€”â€”â€”â€”â€”â€”â€”
conf = st.sidebar.slider("ç½®ä¿¡åº¦é˜ˆå€¼", 0.0, 1.0, 0.5, 0.01)

# â€”â€”â€”â€”â€”â€”â€”â€” å¯åŠ¨ WebRTC æµ â€”â€”â€”â€”â€”â€”â€”â€”
webrtc_ctx = webrtc_streamer(
    key="yolo-webrtc",
    mode="SENDRECV",
    rtc_configuration={
        "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
    },
    video_transformer_factory=VideoTransformer,
)

# åŠ¨æ€ä¿®æ”¹ç½®ä¿¡åº¦
if webrtc_ctx.video_transformer:
    webrtc_ctx.video_transformer.conf_th = conf

st.sidebar.markdown(
    """
    **ä½¿ç”¨è¯´æ˜**  
    1. è¿è¡Œåæµè§ˆå™¨ä¼šè¯·æ±‚â€œå…è®¸è®¿é—®æ‘„åƒå¤´â€ã€‚  
    2. ç‚¹å‡»â€œå…è®¸â€å³å¯çœ‹åˆ°ç”»é¢å¹¶å¼€å§‹å®æ—¶æ£€æµ‹ã€‚  
    3. å¯åœ¨ä¾§æ è°ƒæ•´ Detection Confidenceã€‚  
    """
)
